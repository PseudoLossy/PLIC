{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Danish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Danish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Danish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Danish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating similarities:   0%|          | 0/87 [00:00<?, ?it/s]c:\\Users\\Danish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:491: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Calculating similarities:   0%|          | 0/87 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 154\u001b[0m\n\u001b[0;32m    151\u001b[0m vgg_similarity \u001b[38;5;241m=\u001b[39m cosine_similarity(vgg_features1, vgg_features2)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Compute SSIM, MSE, LPIPS, and PSNR\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m ssim_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_ssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1_resized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2_resized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m mse_score \u001b[38;5;241m=\u001b[39m compute_mse(img1_resized, img2_resized)\n\u001b[0;32m    156\u001b[0m lpips_similarity \u001b[38;5;241m=\u001b[39m compute_lpips(img1_resized, img2_resized)\n",
      "Cell \u001b[1;32mIn[2], line 74\u001b[0m, in \u001b[0;36mcompute_ssim\u001b[1;34m(image1, image2)\u001b[0m\n\u001b[0;32m     72\u001b[0m image1_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image1)\n\u001b[0;32m     73\u001b[0m image2_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image2)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultichannel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Danish\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skimage\\metrics\\_structural_similarity.py:178\u001b[0m, in \u001b[0;36mstructural_similarity\u001b[1;34m(im1, im2, win_size, gradient, data_range, channel_axis, gaussian_weights, full, **kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m         win_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m   \u001b[38;5;66;03m# backwards compatibility\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many((np\u001b[38;5;241m.\u001b[39masarray(im1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m win_size) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin_size exceeds image extent. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEither ensure that your images are \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat least 7x7; or pass win_size explicitly \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min the function call, with an odd value \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mless than or equal to the smaller side of your \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages. If your images are multichannel \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(with color channels), set channel_axis to \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe axis number corresponding to the channels.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (win_size \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWindow size must be odd.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: win_size exceeds image extent. Either ensure that your images are at least 7x7; or pass win_size explicitly in the function call, with an odd value less than or equal to the smaller side of your images. If your images are multichannel (with color channels), set channel_axis to the axis number corresponding to the channels."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPModel, AutoImageProcessor, AutoModel\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lpips\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load CLIP model and processor\n",
    "processor_clip = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "# Load DINOv2 model and processor\n",
    "processor_dino = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model_dino = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "\n",
    "# Load VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "\n",
    "# Load LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "# Image transformation for VGG16 and LPIPS\n",
    "vgg_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "lpips_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Function to extract CLIP features\n",
    "def extract_features_clip(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_clip(images=image, return_tensors=\"pt\").to(device)\n",
    "        image_features = model_clip.get_image_features(**inputs)\n",
    "        return image_features\n",
    "\n",
    "# Function to extract DINOv2 features\n",
    "def extract_features_dino(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_dino(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model_dino(**inputs)\n",
    "        image_features = outputs.last_hidden_state\n",
    "        return image_features.mean(dim=1)\n",
    "\n",
    "# Function to extract VGG16 features\n",
    "def extract_features_vgg16(image):\n",
    "    image = vgg_transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = vgg16(image).flatten(1)\n",
    "    return features\n",
    "\n",
    "# Function to calculate cosine similarity between two feature vectors\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    vector1 = vector1 / vector1.norm(dim=-1, keepdim=True)\n",
    "    vector2 = vector2 / vector2.norm(dim=-1, keepdim=True)\n",
    "    return torch.sum(vector1 * vector2, dim=-1).item()\n",
    "\n",
    "# Function to compute SSIM between two images\n",
    "def compute_ssim(image1, image2):\n",
    "    image1_np = np.array(image1)\n",
    "    image2_np = np.array(image2)\n",
    "    return ssim(image1_np, image2_np, multichannel=True)\n",
    "\n",
    "# Function to compute MSE between two images\n",
    "def compute_mse(image1, image2):\n",
    "    image1_np = np.array(image1).astype(np.float32)\n",
    "    image2_np = np.array(image2).astype(np.float32)\n",
    "    return mean_squared_error(image1_np.flatten(), image2_np.flatten())\n",
    "\n",
    "# Function to compute LPIPS (1 - LPIPS for similarity score)\n",
    "def compute_lpips(image1, image2):\n",
    "    # Transform images for LPIPS\n",
    "    img1_tensor = lpips_transform(image1).unsqueeze(0).to(device)\n",
    "    img2_tensor = lpips_transform(image2).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        lpips_score = lpips_model(img1_tensor, img2_tensor).item()\n",
    "    return 1 - lpips_score  # Since LPIPS measures distance, we use 1 - LPIPS for similarity\n",
    "\n",
    "# Function to compute PSNR between two images\n",
    "def compute_psnr(image1, image2):\n",
    "    image1_np = np.array(image1).astype(np.float32)\n",
    "    image2_np = np.array(image2).astype(np.float32)\n",
    "    return psnr(image1_np, image2_np)\n",
    "\n",
    "# Retrieve all filenames from the two directories\n",
    "def get_image_paths(directory):\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('jpg'):\n",
    "                images.append(os.path.join(root, file))\n",
    "    images.sort()  # Ensure images are in the same order in both directories\n",
    "    return images\n",
    "\n",
    "# Resize images to the minimum dimensions between two images\n",
    "def resize_to_minimum(img1, img2):\n",
    "    min_width = min(img1.width, img2.width)\n",
    "    min_height = min(img1.height, img2.height)\n",
    "    img1_resized = img1.resize((min_width, min_height))\n",
    "    img2_resized = img2.resize((min_width, min_height))\n",
    "    return img1_resized, img2_resized\n",
    "\n",
    "# Paths to the two directories containing the images\n",
    "dir1 = './images_dataset/'\n",
    "dir2 = './images_recon/'\n",
    "\n",
    "# Get the sorted image paths for both directories\n",
    "images1 = get_image_paths(dir1)\n",
    "images2 = get_image_paths(dir2)\n",
    "\n",
    "# Ensure both directories have the same number of images\n",
    "assert len(images1) == len(images2), \"The directories do not contain the same number of images.\"\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over the paired images and calculate similarity scores\n",
    "for img1_path, img2_path in tqdm(zip(images1, images2), total=len(images1), desc=\"Calculating similarities\"):\n",
    "    # Open images\n",
    "    img1 = Image.open(img1_path).convert('RGB')\n",
    "    img2 = Image.open(img2_path).convert('RGB')\n",
    "    \n",
    "    # Resize images to the minimum dimensions\n",
    "    img1_resized, img2_resized = resize_to_minimum(img1, img2)\n",
    "    \n",
    "    # Extract features using CLIP\n",
    "    clip_features1 = extract_features_clip(img1_resized).to(device)\n",
    "    clip_features2 = extract_features_clip(img2_resized).to(device)\n",
    "    clip_similarity = cosine_similarity(clip_features1, clip_features2)\n",
    "    \n",
    "    # Extract features using DINOv2\n",
    "    dino_features1 = extract_features_dino(img1_resized).to(device)\n",
    "    dino_features2 = extract_features_dino(img2_resized).to(device)\n",
    "    dino_similarity = cosine_similarity(dino_features1, dino_features2)\n",
    "    \n",
    "    # Extract features using VGG16\n",
    "    vgg_features1 = extract_features_vgg16(img1_resized).to(device)\n",
    "    vgg_features2 = extract_features_vgg16(img2_resized).to(device)\n",
    "    vgg_similarity = cosine_similarity(vgg_features1, vgg_features2)\n",
    "    \n",
    "    # Compute SSIM, MSE, LPIPS, and PSNR\n",
    "    ssim_score = compute_ssim(img1_resized, img2_resized)\n",
    "    mse_score = compute_mse(img1_resized, img2_resized)\n",
    "    lpips_similarity = compute_lpips(img1_resized, img2_resized)\n",
    "    psnr_score = compute_psnr(img1_resized, img2_resized)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"image1\": os.path.relpath(img1_path, start='.'),\n",
    "        \"image2\": os.path.relpath(img2_path, start='.'),\n",
    "        \"CLIP_Similarity\": clip_similarity,\n",
    "        \"DINOv2_Similarity\": dino_similarity,\n",
    "        \"VGG16_Similarity\": vgg_similarity,\n",
    "        \"SSIM\": ssim_score,\n",
    "        \"MSE\": mse_score,\n",
    "        \"1-LPIPS\": lpips_similarity,\n",
    "        \"PSNR\": psnr_score\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"userstudy2.csv\", index=False)\n",
    "\n",
    "print(\"Similarity metrics saved to image_similarity_metrics.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
